{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb97c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "from dataset.mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39f33b",
   "metadata": {},
   "source": [
    "# 5.2 連鎖律\n",
    "- 合成関数の微分の性質\n",
    "- 高速に勾配を計算する誤差逆伝播法に応用する\n",
    "- ニューラルネット全体の関数を，各ノードの局所的な関数とその先の様々な関数の合成関数と考えて応用\n",
    "\n",
    "$$\n",
    "z = f(t) \\\\\n",
    "t = g(x, y)\n",
    "$$\n",
    "とすると，次のように合成関数の微分を求めることができる\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153ed17",
   "metadata": {},
   "source": [
    "### 単純な乗算レイヤ，加算レイヤ\n",
    "\n",
    "- 単純な乗算レイヤの例．\n",
    "- 様々な複雑な演算を含む損失関数を $ L(z) $ とする．\n",
    "- その内，一つの乗算ノードで $ z = xy $ という演算が行われるとする．\n",
    "\n",
    "このとき，次のように勾配を求めることができる\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial x} = \\frac{\\partial L}{\\partial z} y \\\\　\\\\\n",
    "\\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial y} = \\frac{\\partial L}{\\partial z} x\n",
    "$$\n",
    "- $ \\frac{\\partial L}{\\partial z} $ は乗算ノードの出力 $ z $ での $ L $ の偏微分．\n",
    "- $ x, y $ を勾配計算に用いるため，順伝播の入力を保持しておく必要がある．\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6000a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MulLayer:\n",
    "#     def __init__(self):\n",
    "#         self.x = None\n",
    "#         self.y = None\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "#         out = x * y\n",
    "#         return out\n",
    "    \n",
    "#     def backward(self, dout):\n",
    "#         dx = dout * self.y\n",
    "#         dy = dout * self.x\n",
    "#         return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a945dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AddLayer:\n",
    "#     def __init__(self):\n",
    "#         self.x = None\n",
    "#         self.y = None\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "#         out = x + y\n",
    "        \n",
    "#         return out\n",
    "    \n",
    "#     def backward(self, dout):\n",
    "#         dx = dout\n",
    "#         dy = dout\n",
    "        \n",
    "#         return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7b7b7",
   "metadata": {},
   "source": [
    "# 5.5 活性化関数レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0321e",
   "metadata": {},
   "source": [
    "### ReLUレイヤ\n",
    "#### ReLU\n",
    "$$\n",
    "y = \n",
    "\\begin{cases}\n",
    "    x \\quad (x \\gt 0) \\\\\n",
    "    0 \\quad (x \\leq 0)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### ReLUの偏微分\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \n",
    "\\begin{cases}\n",
    "    1 \\quad (x \\gt 0) \\\\\n",
    "    0 \\quad (x \\leq 0)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888fed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Relu:\n",
    "#     def __init__(self):\n",
    "#         self.mask = None\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # 入力が 0 以下のインデックスを保持し，その値は0とする\n",
    "#         self.mask = (x <= 0)\n",
    "#         out = x.copy()\n",
    "#         out[self.mask] = 0\n",
    "        \n",
    "#         return out\n",
    "    \n",
    "#     def backward(self, dout):\n",
    "#         # 順伝播の入力が 0 以下であったインデックスの値は 0 ，そのほかはそのまま逆伝播\n",
    "#         dout[self.mask] = 0\n",
    "#         dx = dout\n",
    "        \n",
    "#         return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7183a0",
   "metadata": {},
   "source": [
    "### Sigmoidレイヤ\n",
    "#### Sigmoid\n",
    "$$\n",
    "y = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "#### Sigmoidの偏微分導出\n",
    "$$\n",
    "t = 1 + \\exp(-x)\n",
    "$$\n",
    "\n",
    "とおくと，\n",
    "$$\n",
    "y = \\frac{1}{1 + \\exp(-x)} = \\frac{1}{t} = t^{-1}\n",
    "$$\n",
    "\n",
    "また，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial t} &= -t^{-2} = -(1 + \\exp(-x))^{-2}\\\\\n",
    "\\frac{\\partial t}{\\partial x} &= -\\exp(-x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "であるから，\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial x} &= \\frac{\\partial y}{\\partial t} \\frac{\\partial t}{\\partial x} \\\\ \\\\\n",
    "&= \\frac{\\exp(-x)}{\\{1 + \\exp(-x)\\}^{2}} \\\\ \\\\\n",
    "&= \\frac{\\exp(-x)}{1 + \\exp(-x)} \\frac{1}{1 + \\exp(-x)} \\\\ \\\\\n",
    "&= \\left(\\frac{1 + \\exp(-x)}{1 + \\exp(-x)} - \\frac{1}{1 + \\exp(-x)} \\right) \\frac{1}{1 + \\exp(-x)} \\\\ \\\\\n",
    "&= (1 - y) y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b64cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sigmoid:\n",
    "#     def __init__(self):\n",
    "#         self.out = None\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = 1 / (1 + np.exp(-x))\n",
    "#         self.out = out\n",
    "        \n",
    "#         return out\n",
    "    \n",
    "#     def backward(self, dout):\n",
    "#         # 逆伝播の入力に，Sigmoidの偏微分をかける\n",
    "#         dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "#         return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe0a33",
   "metadata": {},
   "source": [
    "# 5.6 Affine / Softmaxレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81966205",
   "metadata": {},
   "source": [
    "### Affine変換\n",
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{x} \\cdot W + \\boldsymbol{b}\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol{y}, \\ \\boldsymbol{x}, \\ \\boldsymbol{b}$ はベクトル，$W$ は行列\n",
    "- 順伝播における，入力 $ \\boldsymbol{x} $，重み $ W $，バイアス $ \\boldsymbol{b} $ を表す\n",
    "- 損失関数 $ L(Y) (\\in \\boldsymbol{R}) $ の変数と考える\n",
    "\n",
    "***\n",
    "#### Affineレイヤの逆伝播\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{x}} = \\frac{\\partial L}{\\partial \\boldsymbol{y}} W^T \\\\ \\\\\n",
    "\\frac{\\partial L}{\\partial W} = \\boldsymbol{x}^T \\frac{\\partial L}{\\partial \\boldsymbol{y}} \\\\ \\\\\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "#### 導出\n",
    "- 第 $(l-1)$ 層から第 $l$ 層への変換を考える\n",
    "- 以下のように定義\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{y} &= \\left( y_1, y_2, \\cdots, y_j, \\cdots, y_m \\right)\n",
    "\\\\ \\\\\n",
    "\\boldsymbol{x} &= \\left( x_1, x_2, \\cdots, x_k, \\cdots, x_n \\right)\n",
    "\\\\ \\\\\n",
    "W &= \\begin{pmatrix}\n",
    "w_{11} & w_{21} & \\cdots & w_{m1} \\\\\n",
    "w_{12} & w_{22} & \\cdots & w_{m2} \\\\\n",
    "\\vdots & \\vdots & w_{jk} & \\vdots \\\\\n",
    "w_{1n} & w_{2n} & \\cdots & w_{mn}\n",
    "\\end{pmatrix}\n",
    "\\\\ \\\\\n",
    "\\boldsymbol{b} &= \\left( b_1, b_2, \\cdots, b_j, \\cdots, b_m \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "- $n$: $(l-1)$ 層のノード数\n",
    "- $m$: $l$ 層のノード数\n",
    "- $\\boldsymbol{y}$: Affine変換後の $l$ 層への入力．\n",
    "- $\\boldsymbol{x}$: $(l-1)$ 層からの出力．\n",
    "- $W$: 重み．$w_{jk}$ は $(l-1)$ 層 $k$ 番目のノードから $l$ 層 $j$ 番目のノードへの重み．\n",
    "- $\\boldsymbol{b}$: バイアス．\n",
    "\n",
    "***\n",
    "\n",
    "Affine変換を成分表示\n",
    "\n",
    "$$\\boldsymbol{y} = \\left( x_1, x_2, \\cdots, x_k, \\cdots, x_n \\right)\n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{21} & \\cdots & w_{m1} \\\\\n",
    "w_{12} & w_{22} & \\cdots & w_{m2} \\\\\n",
    "\\vdots & \\vdots & w_{jk} & \\vdots \\\\\n",
    "w_{1n} & w_{2n} & \\cdots & w_{mn}\n",
    "\\end{pmatrix}\n",
    "+ \\left( b_1, b_2, \\cdots, b_j, \\cdots, b_m \\right)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol{y}$ の成分 $y_j$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_j \n",
    "&= x_1 w_{j1} + x_2 w_{j2} + \\cdots  + x_n w_{jn} + b_j \\\\ \\\\\n",
    "&= \\Sigma_{k=1}^{n} x_k w_{jk} + b_j\n",
    "\\end{align}\n",
    "$$\n",
    "- $y_j$ は $x_k, \\ w_{jk}, \\ b_j, (k = 1, 2, \\cdots, n)$ の関数\n",
    "- $W, \\ \\boldsymbol{b}$ の $j$ 列目以外の成分は関係ない\n",
    "\n",
    "\n",
    "$\\boldsymbol{y}$ の成分 $y_j$ の偏微分\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y_j}{\\partial x_k} \n",
    "&= w_{jk}\n",
    "\\\\ \\\\\n",
    "\\frac{\\partial y_j}{\\partial w_{jk}} \n",
    "&= x_k\n",
    "\\\\ \\\\\n",
    "\\frac{\\partial y_j}{\\partial b_j} \n",
    "&= 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "損失関数の $L$ の $\\boldsymbol{x}$ による微分\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial x_k}\n",
    "&= \\frac{\\partial L}{\\partial y_1} \\frac{\\partial y_1}{\\partial x_k} + \\frac{\\partial L}{\\partial y_2} \\frac{\\partial y_2}{\\partial x_k} + \\cdots + \\frac{\\partial L}{\\partial y_m} \\frac{\\partial y_m}{\\partial x_k}\n",
    "\\\\ \\\\\n",
    "&= \\left( \\frac{\\partial L}{\\partial y_1}, \\frac{\\partial L}{\\partial y_2}, \\cdots, \\frac{\\partial L}{\\partial y_m} \\right)\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\frac{\\partial y_1}{\\partial x_k} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_k} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_k}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\\\ \\\\\n",
    "&= \\frac{\\partial L}{\\partial \\boldsymbol{y}}\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "w_{1k}\\\\\n",
    "w_{2k} \\\\\n",
    "\\vdots \\\\\n",
    "w_{mk}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$k = 1, 2, \\cdots, n$ をまとめてベクトルにする\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{x}}\n",
    "&= \\left(\n",
    "\\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2}, \\cdots, \\frac{\\partial L}{\\partial x_n}\n",
    "\\right)\n",
    "\\\\ \\\\\n",
    "&= \\frac{\\partial L}{\\partial \\boldsymbol{y}}\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "w_{11} & w_{12} & \\cdots & w_{1n} \\\\\n",
    "w_{21} & w_{22} & \\cdots & w_{2n} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "w_{m1} & w_{m2} & \\cdots & w_{mn}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\\\ \\\\\n",
    "&= \\frac{\\partial L}{\\partial \\boldsymbol{y}} W^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "損失関数の $L$ の $W$ による微分\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_{jk}}\n",
    "&= \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial w_{jk}}\n",
    "\\\\ \\\\\n",
    "&= \\frac{\\partial L}{\\partial y_j} x_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$ j = 1, 2, \\cdots, m, k = 1, 2, \\cdots, n $ をまとめて行列にする\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W}\n",
    "&= \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{21}} & \\cdots &\\frac{\\partial L}{\\partial w_{m1}} \\\\\n",
    "\\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{22}} & \\cdots &\\frac{\\partial L}{\\partial w_{m2}} \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial w_{1n}} & \\frac{\\partial L}{\\partial w_{2n}} & \\cdots &\\frac{\\partial L}{\\partial w_{mn}}\n",
    "\\end{pmatrix}\n",
    "\\\\ \\\\\n",
    "&= \\begin{pmatrix}\n",
    "\\frac{\\partial L}{\\partial y_1} x_1 & \\frac{\\partial L}{\\partial y_2} x_1 & \\cdots &\\frac{\\partial L}{\\partial y_m} x_1 \\\\\n",
    "\\frac{\\partial L}{\\partial y_1} x_2 & \\frac{\\partial L}{\\partial y_2} x_2 & \\cdots &\\frac{\\partial L}{\\partial y_m} x_2 \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial y_1} x_n & \\frac{\\partial L}{\\partial y_2} x_n & \\cdots &\\frac{\\partial L}{\\partial y_m} x_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\\\ \\\\\n",
    "&= \\left( \\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array} \\right)\n",
    "\\left( \\frac{\\partial L}{\\partial y_1}, \\frac{\\partial L}{\\partial y_2}, \\cdots, \\frac{\\partial L}{\\partial y_m} \\right)\n",
    "\\\\ \\\\\n",
    "&= \\boldsymbol{x}^T \\frac{\\partial L}{\\partial \\boldsymbol{y}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***\n",
    "\n",
    "補足\n",
    "\n",
    "$\\boldsymbol{v}_j = \\left( w_{j1}, w_{j2}, \\cdots, w_{jn} \\right)^T$ とおくと\n",
    "$W = \\left( \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\cdots, \\boldsymbol{v}_m \\right)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial W}\n",
    "&= \\left( \\frac{\\partial L}{\\partial \\boldsymbol{v}_1}, \\frac{\\partial L}{\\partial \\boldsymbol{v}_2}, \\cdots, \\frac{\\partial L}{\\partial \\boldsymbol{v}_m} \\right)\n",
    "\\\\ \\\\\n",
    "&= \\left( \\frac{\\partial L}{\\partial \\boldsymbol{y}_1} \\frac{\\partial \\boldsymbol{y}_1}{\\partial \\boldsymbol{v}_1}, \\frac{\\partial L}{\\partial \\boldsymbol{y}_2} \\frac{\\partial \\boldsymbol{y}_2}{\\partial \\boldsymbol{v}_2}, \\cdots, \\frac{\\partial L}{\\partial \\boldsymbol{y}_m} \\frac{\\partial \\boldsymbol{y}_m}{\\partial \\boldsymbol{v}_m} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d6df1",
   "metadata": {},
   "source": [
    "#### バッチ対応Affineレイヤの実装\n",
    "- 重み $W$ と，バイアス $\\boldsymbol{b}$ を渡してインスタンスを生成\n",
    "- 入力 $\\boldsymbol{x}$ を渡して順伝播\n",
    "- $\\frac{\\partial L}{\\partial \\boldsymbol{y}}$ を渡して逆伝播\n",
    "- $\\frac{\\partial L}{\\partial \\boldsymbol{b}}$ はaxis=0で総和を求める\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c529f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Affine:\n",
    "#     def __init__(self, W, b):\n",
    "#         self.W = W\n",
    "#         self.b = b\n",
    "#         self.x = None\n",
    "#         self.dW = None\n",
    "#         self.db = None\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         self.x = x\n",
    "#         out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "#         return out\n",
    "    \n",
    "#     def backward(self, dout):\n",
    "#         dx = np.dot(dout, self.W.T)\n",
    "#         self.dW = np.dot(self.x.T, dout)\n",
    "#         self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "#         return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29013ff5",
   "metadata": {},
   "source": [
    "#### Softmax-with-Lossレイヤ\n",
    "- $n$ クラスの分類問題を考える（10種の手書き文字認識は $n = 10$）\n",
    "- Softmaxレイヤへの入力は $n$ 個\n",
    "- Softmaxレイヤは入力された値を正規化して出力する\n",
    "- ここでは損失関数の交差エントロピー誤差を含めたレイヤを考える\n",
    "\n",
    "***\n",
    "\n",
    "#### Softmax関数\n",
    "$$\n",
    "y_k = \\frac{\\exp(a_k)}{\\Sigma_{i=1}^n \\exp(a_i)}\n",
    "$$\n",
    "- $n$: 分類するクラス数\n",
    "- $\\boldsymbol{a} = (a_1, a_2, \\cdots, a_n)$: 前レイヤからの入力\n",
    "- $\\boldsymbol{y} = (y_1, y_2, \\cdots, y_n)$: Softmax関数の出力．$\\Sigma_{k=1}^{n} y_k = 1$\n",
    "\n",
    "#### Cross Entropy Error\n",
    "$$\n",
    "L = - \\Sigma_{k=1}^n \\ t_k \\log y_k\n",
    "$$\n",
    "- $\\boldsymbol{t} = (t_1, t_2, \\cdots, t_n)$: 教師ラベル．one-hot表現．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b371b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Cross Entropy Error $L$ を $y_k$ で偏微分\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_k} = - \\frac{t_k}{y_k} \n",
    "$$\n",
    "\n",
    "#### Softmax関数 $y_k$ を $a_j$ で偏微分\n",
    "以下，$S = \\Sigma_{i=1}^n \\exp(a_i)$ とおく\n",
    "\n",
    "次の微分を使う\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\exp(a)^{\\prime} &= \\exp(a)\n",
    "\\\\ \\\\\n",
    "\\frac{\\partial S}{\\partial a_j} \n",
    "&= \\Sigma_{i=1}^n \\frac{\\partial \\exp(a_i)}{\\partial a_j}\n",
    "\\\\ \\\\\n",
    "&= \\begin{cases}\n",
    "\\exp(a_i) & (i = j) \\\\\n",
    "0 & (i \\ne j)\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "商の微分公式より\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_k}{\\partial a_j} = \\frac{\\exp(a_k)^{\\prime} S - \\exp(a_k) S^{\\prime}}{S}\n",
    "$$\n",
    "\n",
    "- $j = k$ の場合\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y_k}{\\partial a_k}\n",
    "&= \\frac{\\exp(a_k) S - \\exp(a_k) \\exp(a_k) }{S^2} \\\\ \\\\\n",
    "&= \\frac{\\exp(a_k)}{S} \\left( 1 - \\frac{\\exp(a_k)}{S} \\right) \\\\ \\\\\n",
    "&= y_k (1 - y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $j \\ne k$ の場合\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y_k}{\\partial a_j}\n",
    "&= \\frac{- \\exp(a_k) \\exp(a_j) }{S^2} \\\\ \\\\\n",
    "&= - y_k y_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Cross Entropy ErrorとSoftmax関数の合成関数の微分\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial a_k}\n",
    "&= \\Sigma_{i=1}^n \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial a_k} \\\\ \\\\\n",
    "&= \\left(-\\frac{t_1}{y_1}\\right)(-y_1 y_k) + \\left(-\\frac{t_2}{y_2}\\right)(-y_2 y_k) + \\cdots + \\left(-\\frac{t_k}{y_k}\\right) y_k(1 - y_k) + \\cdots + \\left(-\\frac{t_n}{y_n}\\right)(-y_n y_k) \\\\ \\\\\n",
    "&= t_1 y_k + t_2 y_k + \\cdots + t_k y_k + \\cdots + t_n y_k - t_k \\\\ \\\\\n",
    "&= \\left(\\Sigma_{i=1}^n \\ t_i\\right) y_k - t_k \\qquad \\left(\\Sigma_{i=1}^n \\ t_i = 1 \\right) \\\\ \\\\\n",
    "&= y_k - t_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$t_i \\ (i = 1, 2, \\cdots, n)$ はone-hot表現の教師データため，総和は1になる\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a93a295",
   "metadata": {},
   "source": [
    "#### Softmax with Lossレイヤ実装\n",
    "- 損失関数の値，softmaxの出力，one-hot vectorの教師データをインスタンス変数として保持する\n",
    "- forwardメソッドにレイヤへの入力と教師データを渡すと損失関数の値を返す\n",
    "- backwardメソッドはSoftmaxレイヤの前のレイヤの勾配を返す\n",
    "- backwardメソッドではバッチ数で割ることで，1データあたりの勾配を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9f9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SoftmaxWithLoss:\n",
    "#     def __init__(self):\n",
    "#         self.loss = None # 損失\n",
    "#         self.y = None    # softmaxの出力\n",
    "#         self.t = None    # 教師データ one-hot vector\n",
    "        \n",
    "#     def forward(self, x, t):\n",
    "#         self.t = t\n",
    "#         self.y = softmax(x)\n",
    "#         self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "#         return self.loss\n",
    "    \n",
    "#     def backward(self, dout=1):\n",
    "#         batch_size = self.t.shape[0]\n",
    "#         dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "#         return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22946377",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea68e5",
   "metadata": {},
   "source": [
    "# 5.7 誤差逆伝播法を使ったニューラルネットワーク実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e10cb6",
   "metadata": {},
   "source": [
    "### 2層ニューラルネットワークを実装\n",
    "#### TwoLayerNetクラスを実装する\n",
    "- params: パラメータ(重み，バイアス)を保持するインスタンス変数．dict\n",
    "- layers: レイヤを保持するインスタンス変数．ordered dict\n",
    "- lastLayer: ニューラルネットワークの最後のレイヤ．SoftmaxWithLoss\n",
    "- __init__: 入力層のニューロン数，隠れ層のニューロン数，重み初期化時のガウス分布のスケールを引数にとる\n",
    "- predict: 画像データを引数にとる．推論を行う．予測したクラスを出力\n",
    "- loss: 画像データ，正解ラベルを引数にとり，損失関数の値を返す\n",
    "- accuracy: 画像データ，正解ラベルを引数にとり，認識精度(正解した割合)を返す\n",
    "- numerical_gradient: 画像データ，正解ラベルを引数にとる．重みパラメータに対する勾配を数値微分によって求める．gradientメソッドの確認に使う\n",
    "- gradient: 画像データ，正解ラベルを引数にとる．重みパラメータに対する勾配を誤差逆伝播法によって求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "949824c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) # 入力層から中間層\n",
    "        self.layers['Relu1'] = Relu()                                         # 中間層の活性化関数\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2']) # 中間層から出力層\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()                                    # 出力層\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 各レイヤのforwardメソッドを順に適用していく\n",
    "        # 予測のみ(出力最大のインデックスが得られれば良い)なので，softmax, cross entropy error は不要\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)   \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        # 予測値と正解ラベルから損失関数の値を計算する\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        # numerical_gradient関数に渡すための関数を用意\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        # numerical_gradient関数で勾配を求める\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1']) # self.params['W1']におけるloss_Wの数値微分を求めている\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        loss = self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout) # 出力層\n",
    "        # 各レイヤのbackwardメソッドを逆順に適用していく\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout) # Affineレイヤのインスタンス変数dW, dbが更新される\n",
    "            \n",
    "        # 更新後のAffineレイヤのインスタンス変数を参照して勾配を返す\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0208b9e",
   "metadata": {},
   "source": [
    "***\n",
    "### 誤差逆伝播法の勾配確認\n",
    "TwoLayerNetクラスの逆伝播を使ったgradientメソッドによる勾配計算が正しいか，数値微分と比較する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a83790",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f66a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 4.390436293698236e-10\n",
      "b1: 2.696124428356022e-09\n",
      "W2: 5.126298535588114e-09\n",
      "b2: 1.4004944885043225e-07\n"
     ]
    }
   ],
   "source": [
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + ': ' + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a2e7b",
   "metadata": {},
   "source": [
    "***\n",
    "### 誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c5cb707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.10035 0.1004\n",
      "600 0.9030666666666667 0.9055\n",
      "1200 0.9218666666666666 0.9262\n",
      "1800 0.9352 0.9368\n",
      "2400 0.9429333333333333 0.9426\n",
      "3000 0.9509166666666666 0.9496\n",
      "3600 0.95545 0.9523\n",
      "4200 0.9605 0.9583\n",
      "4800 0.9625833333333333 0.9586\n",
      "5400 0.96665 0.963\n",
      "6000 0.9691 0.9631\n",
      "6600 0.97065 0.9639\n",
      "7200 0.9731 0.9675\n",
      "7800 0.9748333333333333 0.9687\n",
      "8400 0.9759833333333333 0.9691\n",
      "9000 0.9776833333333333 0.9701\n",
      "9600 0.9778333333333333 0.9694\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key, value in grad.items():\n",
    "        network.params[key] -= learning_rate * value\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(i, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed702e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
